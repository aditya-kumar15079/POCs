metrics:
  bleu:
    enabled: true
    weight: 0.05  # 15% weight in overall score
    thresholds:
      good: 0.7  # Scores >= 0.7 are considered good
      acceptable: 0.5  # Scores >= 0.5 and < 0.7 are acceptable
      poor: 0.0  # Scores < 0.5 are poor
    description: "Measures n-gram overlap between generated and reference text"
    dependencies: ["Reference_Response", "Generated_Response"]

  rouge:
    enabled: true
    weight: 0.10  # 15% weight in overall score
    thresholds:
      good: 0.6
      acceptable: 0.4
      poor: 0.0
    description: "Evaluates recall-oriented understanding for gisting evaluation"
    dependencies: ["Reference_Response", "Generated_Response"]

  meteor:
    enabled: true
    weight: 0.15  # 15% weight in overall score
    thresholds:
      good: 0.6
      acceptable: 0.4
      poor: 0.0
    description: "Considers stemming, synonymy, and word order for text comparison"
    dependencies: ["Reference_Response", "Generated_Response"]

  bert_score:
    enabled: true
    weight: 0.15  # 20% weight in overall score
    thresholds:
      good: 0.8
      acceptable: 0.6
      poor: 0.0
    description: "Uses BERT embeddings to measure semantic similarity"
    dependencies: ["Reference_Response", "Generated_Response"]

  accuracy:
    enabled: true
    weight: 0.25  # 10% weight in overall score
    thresholds:
      good: 0.8
      acceptable: 0.6
      poor: 0.0
    description: "Measures factual correctness of the generated response"
    dependencies: ["Reference_Response", "Generated_Response"]

  answer_relevance:
    enabled: true
    weight: 0.2 # 15% weight in overall score
    thresholds:
      good: 0.8
      acceptable: 0.6
      poor: 0.0
    description: "Evaluates how well the response addresses the given prompt"
    dependencies: ["Prompt", "Generated_Response", "Context"]

  toxicity:
    enabled: true
    weight: 0.10  # 10% weight in overall score
    thresholds:
      good: 0.2  # Lower toxicity scores are better
      acceptable: 0.5
      poor: 1.0
    description: "Measures potential harmful or toxic content in responses"
    dependencies: ["Generated_Response"]
    reverse_scoring: true  # Lower scores are better

reporting:
  output_path: "data/output/evaluation_report.xlsx"
  color_coding:
    good: "92D050"  # Green
    acceptable: "FFC000"  # Yellow
    poor: "FF0000"  # Red
  decimal_places: 4